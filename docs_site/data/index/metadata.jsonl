{"id": "config-reference.md_Training Hyperparameters_0", "text": "## Training Hyperparameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| learning_rate | float | 2e-5 | Initial learning rate |\n| batch_size | int | 30 | Training batch size |\n| gradient_accumulation_steps | int | 3 | Steps between optimizer updates |\n| max_seq_len | int | 512 | Maximum input sequence length |\n| max_target_len | int | 256 | Maximum target sequence length |\n| num_workers | int | 4 | DataLoader worker processes |\n| prefetch_factor | int | 4 | Batches prefetched per worker |\n| num_epochs | int | 50 | Total training epochs |\n| warmup_steps | int | 200 | LR warmup steps |\n| weight_decay | float | 0.01 | AdamW weight decay |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Training Hyperparameters", "start_line": 39, "end_line": 53}}
{"id": "config-reference.md_Model Names_0", "text": "## Model Names\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| modernbert_model | str | answerdotai/ModernBERT-base | Encoder model |\n| gpt2_model | str | google/gemma-3-270m-it | Decoder model |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Model Names", "start_line": 54, "end_line": 60}}
{"id": "config-reference.md_Device and Optimization_0", "text": "## Device and Optimization\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| device | str | cuda | Device for training |\n| use_fp16 | bool | True | Enable FP16 mixed precision |\n| use_bf16 | bool | True | Enable BF16 mixed precision |\n| use_gradient_checkpointing | bool | True | Reduce memory via checkpointing |\n| use_torch_compile | bool | False | Use torch.compile |\n| torch_compile_mode | str | reduce-overhead | Compilation mode |\n| async_checkpointing | bool | True | Save checkpoints async |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Device and Optimization", "start_line": 61, "end_line": 72}}
{"id": "config-reference.md_Attention and Normalization_0", "text": "## Attention and Normalization\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| attn_implementation | str | flash_attention_2 | Attention backend |\n| use_rmsnorm | bool | True | Use RMSNorm |\n| rmsnorm_eps | float | 1e-6 | RMSNorm epsilon |\n| use_prefix_rope | bool | True | RoPE in prefix adapter |\n| rope_base | int | 10000 | RoPE base frequency |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Attention and Normalization", "start_line": 73, "end_line": 82}}
{"id": "config-reference.md_Checkpointing_0", "text": "## Checkpointing\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| checkpoint_dir | str | checkpoints | Directory for checkpoints |\n| save_every | int | 500 | Steps between saves |\n| pretrain_best_model_filename | str | best_model_pretrain.pt | Stage 1-2 output |\n| finetune_best_model_filename | str | best_model_finetune.pt | Stage 3-4 output |\n| handoff_best_model_filename | str | best_model.pt | Final checkpoint |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Checkpointing", "start_line": 83, "end_line": 92}}
{"id": "config-reference.md_Inference_0", "text": "## Inference\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| temperature | float | 0.7 | Sampling temperature |\n| max_generation_length | int | 256 | Max output tokens |\n| do_sample | bool | True | Use sampling vs greedy |\n| top_p | float | 0.9 | Nucleus sampling |\n| top_k | int | 50 | Top-k sampling |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Inference", "start_line": 93, "end_line": 102}}
{"id": "config-reference.md_UI_0", "text": "## UI\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| use_tui | bool | True | Use Rich terminal UI |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "UI", "start_line": 103, "end_line": 108}}
{"id": "config-reference.md_Latent Handling_0", "text": "## Latent Handling\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| normalize_latent | bool | True | L2 normalize before decoding |\n| use_stop_latent | bool | True | Enable stop latent learning |\n| stop_latent_init | str | random_normalized | Initialization method |\n| stop_latent_seed | int | 1337 | Random seed |\n| stop_latent_cosine_threshold | float | 0.99 | Early stopping threshold |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Latent Handling", "start_line": 109, "end_line": 118}}
{"id": "config-reference.md_Datasets_0", "text": "## Datasets\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| dataset_name | str | knkarthick/samsum | Main dataset |\n| train_split | str | train | Train split name |\n| validation_split | str | validation | Validation split name |\n| test_split | str | test | Test split name |\n| max_train_samples | int | None | Limit train samples |\n| wikitext_dataset | str | Hieuman/wikitext-103-filtered | Stage 1 data |\n| arxiv_dataset | str | macrocosm/arxiv_abstracts | Stage 1 data |\n| english_pretrain_dataset | str | shuyuej/English-Pretraining-Dataset | Stage 1 data |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Datasets", "start_line": 119, "end_line": 131}}
{"id": "config-reference.md_Caching_0", "text": "## Caching\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| preprocess_cache_dir | str | cache/preprocessed | Cache location |\n| preprocess_format | str | jsonl | Cache format |\n| skip_preprocessing_if_cached | bool | True | Reuse existing cache |\n| preprocess_batch_size | int | 32 | Batch size for preprocessing |\n| preprocess_validation_fraction | float | 0.05 | Val split for pretraining |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Caching", "start_line": 132, "end_line": 141}}
{"id": "config-reference.md_Cache Field Controls_0", "text": "### Cache Field Controls\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| cache_store_text | bool | False | Store raw text |\n| cache_store_input_ids | bool | False | Store token IDs |\n| cache_store_attention_mask | bool | False | Store attention masks |\n| cache_store_target_ids | bool | True | Store target IDs |\n| cache_store_target_attention_mask | bool | True | Store target masks |\n| cache_store_embeddings_int8 | bool | True | Quantize embeddings |\n| cache_use_embeddings_for_training | bool | True | Use cached embeddings |\n| cache_write_offsets_index | bool | True | Write offset file |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Cache Field Controls", "start_line": 142, "end_line": 154}}
{"id": "config-reference.md_Pipeline Control_0", "text": "## Pipeline Control\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| pipeline_mode | str | two_stage_scientific | Training pipeline type |\n| run_preprocessing | bool | True | Execute preprocessing |\n| run_pretraining | bool | True | Run Stage 1-2 |\n| run_finetuning | bool | True | Run Stage 3-4 |\n| freeze_encoder_compression_in_pipeline | bool | True | Freeze encoder in pipeline |\n| adapter_pretrain_use_middle | bool | False | Use middle in Stage 2 |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Pipeline Control", "start_line": 155, "end_line": 165}}
{"id": "config-reference.md_Stage-Specific Epochs_0", "text": "## Stage-Specific Epochs\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| pretrain_middle_epochs | int | 6 | Stage 1 epochs |\n| pretrain_adapter_epochs | int | 4 | Stage 2 epochs |\n| finetune_middle_epochs | int | 2 | Stage 3 epochs |\n| finetune_adapter_epochs | int | 2 | Stage 4 epochs |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Stage-Specific Epochs", "start_line": 166, "end_line": 174}}
{"id": "config-reference.md_Stage-Specific Batch Sizes_0", "text": "## Stage-Specific Batch Sizes\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| pretrain_middle_batch_size | int | None | Stage 1 batch size |\n| pretrain_adapter_batch_size | int | None | Stage 2 batch size |\n| finetune_middle_batch_size | int | None | Stage 3 batch size |\n| finetune_adapter_batch_size | int | None | Stage 4 batch size |\n\nNote: None means use global batch_size", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Stage-Specific Batch Sizes", "start_line": 175, "end_line": 185}}
{"id": "config-reference.md_Stage-Specific Max Samples_0", "text": "## Stage-Specific Max Samples\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| pretrain_middle_max_samples | int | None | Stage 1 total cap |\n| pretrain_adapter_max_samples | int | None | Stage 2 total cap |\n| finetune_middle_max_samples | int | None | Stage 3 total cap |\n| finetune_adapter_max_samples | int | None | Stage 4 total cap |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Stage-Specific Max Samples", "start_line": 186, "end_line": 194}}
{"id": "config-reference.md_Per-Dataset Caps (Pretraining)_0", "text": "## Per-Dataset Caps (Pretraining)\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| pretrain_middle_max_samples_wikitext | int | None | WikiText cap |\n| pretrain_middle_max_samples_arxiv | int | None | arXiv cap |\n| pretrain_middle_max_samples_english | int | None | English pretrain cap |\n| pretrain_adapter_max_samples_wikitext | int | 150000 | Stage 2 WikiText cap |\n| pretrain_adapter_max_samples_arxiv | int | 150000 | Stage 2 arXiv cap |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Per-Dataset Caps (Pretraining)", "start_line": 195, "end_line": 204}}
{"id": "config-reference.md_Loss Weights_0", "text": "## Loss Weights\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| latent_mse_weight | float | 1.0 | MSE component weight |\n| contrastive_weight | float | 0.1 | Contrastive loss weight |", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Loss Weights", "start_line": 205, "end_line": 211}}
{"id": "config-reference.md_Diagnostic Modes_0", "text": "## Diagnostic Modes\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| training_phase | str | normal | Training mode |\n\nOptions:\n- normal: Standard training\n- phase1: Decoder alignment only\n- phase2: Encoder training\n- phase3: Middle model only", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Diagnostic Modes", "start_line": 212, "end_line": 223}}
{"id": "config-reference.md_Usage Examples_0", "text": "## Usage Examples", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Usage Examples", "start_line": 224, "end_line": 225}}
{"id": "config-reference.md_Creating Config_0", "text": "### Creating Config\n\n```python\nfrom src.config import Config", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Creating Config", "start_line": 226, "end_line": 230}}
{"id": "config-reference.md_Default configuration_0", "text": "# Default configuration\nconfig = Config()", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Default configuration", "start_line": 231, "end_line": 233}}
{"id": "config-reference.md_Modify specific values_0", "text": "# Modify specific values\nconfig.learning_rate = 1e-4\nconfig.batch_size = 16", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Modify specific values", "start_line": 234, "end_line": 237}}
{"id": "config-reference.md_Save and load_0", "text": "# Save and load\nimport json\nwith open('my_config.json', 'w') as f:\n    json.dump(config.__dict__, f)\n```", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Save and load", "start_line": 238, "end_line": 243}}
{"id": "config-reference.md_CLI Override_0", "text": "### CLI Override\n\n```bash\npython main.py --mode train \\\n    --learning-rate 5e-5 \\\n    --batch-size 20 \\\n    --pretrain-middle-epochs 8\n```", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "CLI Override", "start_line": 244, "end_line": 252}}
{"id": "config-reference.md_Configuration Tips_0", "text": "## Configuration Tips\n\n1. Reduce batch_size if out of memory\n2. Increase gradient_accumulation_steps to maintain effective batch size\n3. Adjust warmup_steps based on dataset size\n4. Enable use_bf16 if supported (faster than fp16)\n5. Use async_checkpointing for better throughput\n6. Set max_train_samples for faster experimentation", "metadata": {"source": "config-reference.md", "title": "Configuration Reference", "section": "Configuration Tips", "start_line": 253, "end_line": 261}}
{"id": "models.md_Model Architecture Documentation_0", "text": "# Model Architecture Documentation\n\nThis file documents the core model architecture from src/models.py for the documentation site.", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "Model Architecture Documentation", "start_line": 1, "end_line": 4}}
{"id": "models.md_Overview_0", "text": "## Overview\n\nThe Latent-Space-Model architecture consists of four main components:\n\n1. LatentEncoder: ModernBERT + pooling + projection to latent sequence\n2. MiddleTransformer: Transformer for latent reasoning over sequences\n3. PrefixAdapter: Cross-attention from latent sequence to prefix embeddings\n4. LatentSpaceModel: Combined full model", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "Overview", "start_line": 5, "end_line": 13}}
{"id": "models.md_Core Components_0", "text": "## Core Components", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "Core Components", "start_line": 14, "end_line": 15}}
{"id": "models.md_RMSNorm_0", "text": "### RMSNorm\n\nRoot Mean Square Layer Normalization used throughout the architecture.\n\n```python\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n        x_norm = x / rms\n        return x_norm * self.weight\n```", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "RMSNorm", "start_line": 16, "end_line": 32}}
{"id": "models.md_SwiGLU Activation_0", "text": "### SwiGLU Activation\n\nSwiGLU (SiLU-gated linear unit) activation function used in feed-forward networks.\n\n```python\nclass SwiGLU(nn.Module):\n    def __init__(self, in_dim: int, out_dim: int):\n        super().__init__()\n        self.proj = nn.Linear(in_dim, out_dim * 2)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x_proj = self.proj(x)\n        x_gated, x_linear = x_proj.chunk(2, dim=-1)\n        return F.silu(x_gated) * x_linear\n```", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "SwiGLU Activation", "start_line": 33, "end_line": 48}}
{"id": "models.md_Rotary Embedding (RoPE)_0", "text": "### Rotary Embedding (RoPE)\n\nRotary positional embeddings for position-aware attention.\n\n```python\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim: int, base: int = 10000):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n```", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "Rotary Embedding (RoPE)", "start_line": 49, "end_line": 60}}
{"id": "models.md_LatentEncoder_0", "text": "## LatentEncoder\n\nThe encoder uses ModernBERT to convert text tokens to a compact latent sequence.\n\n```python\nclass LatentEncoder(nn.Module):\n    def __init__(self, model_name: str, hidden_dim: int = 768, latent_dim: int = 256,\n                 latent_seq_len: int = 8, num_unfrozen_layers: int = 0, ...):\n```\n\nThe encoder performs:\n1. ModernBERT encoding: [B, L, 768]\n2. Mean pooling to [B, 768]\n3. Compression MLP to [B, 8, 256]", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "LatentEncoder", "start_line": 61, "end_line": 75}}
{"id": "models.md_MultiHeadSelfAttention_0", "text": "## MultiHeadSelfAttention\n\nSelf-attention mechanism with optional RoPE.\n\nConfiguration:\n- dim: 256 (latent dimension)\n- num_heads: 4\n- head_dim: 64 (256 / 4)\n- use_rope: True\n\nThe attention pattern across 8 latent positions allows each vector to attend to the others.", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "MultiHeadSelfAttention", "start_line": 76, "end_line": 87}}
{"id": "models.md_TransformerBlock_0", "text": "## TransformerBlock\n\nCore building block of the MiddleTransformer.\n\n```python\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim: int, num_heads: int, ffn_multiplier: float = 4.0, ...):\n        self.norm1 = RMSNorm(dim, eps=1e-6)\n        self.attn = MultiHeadSelfAttention(dim, num_heads, use_rope=True)\n        self.norm2 = RMSNorm(dim, eps=1e-6)\n        self.ffn = nn.Sequential(\n            SwiGLU(dim, hidden_dim),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.ffn(self.norm2(x))\n        return x\n```", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "TransformerBlock", "start_line": 88, "end_line": 110}}
{"id": "models.md_MiddleTransformer_0", "text": "## MiddleTransformer\n\nThe core reasoning module operating on latent sequences.\n\n```python\nclass MiddleTransformer(nn.Module):\n    def __init__(\n        self,\n        latent_dim: int = 256,\n        num_layers: int = 4,\n        num_heads: int = 4,\n        ffn_multiplier: float = 4.0,\n        dropout: float = 0.1,\n        use_rope: bool = True,\n        rope_base: int = 10000\n    )\n```\n\nTensor shapes:\n- Input: [B, 8, 256]\n- Output: [B, 8, 256]\n- Internal FFN: 256 * 4 = 1024 hidden units", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "MiddleTransformer", "start_line": 111, "end_line": 133}}
{"id": "models.md_PrefixAdapter_0", "text": "## PrefixAdapter\n\nCross-attention bridge between latent space and decoder embeddings.\n\n```python\nclass PrefixAdapter(nn.Module):\n    def __init__(\n        self,\n        latent_dim: int = 256,\n        decoder_dim: int = 640,\n        prefix_len: int = 50,\n        num_heads: int = 4,\n        dropout: float = 0.1,\n        use_rope: bool = True\n    )\n```\n\nCross-attention flow:\n- Query: [B, 50, 640] (learnable positions)\n- Key/Value: [B, 8, 256] (from latent)\n- Output: [B, 50, 640] (prefix embeddings)", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "PrefixAdapter", "start_line": 134, "end_line": 155}}
{"id": "models.md_LatentSpaceModel_0", "text": "## LatentSpaceModel\n\nFull model combining all components.\n\nData flow:\n1. Encode: text -> ModernBERT -> [B, 8, 256]\n2. Middle: [B, 8, 256] -> TransformerBlock x4 -> [B, 8, 256]\n3. Adapt: latent -> CrossAttention -> [B, 50, 640]\n4. Decode: prefix -> Gemma -> generated text", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "LatentSpaceModel", "start_line": 156, "end_line": 165}}
{"id": "models.md_Stop Latent Mechanism_0", "text": "## Stop Latent Mechanism\n\nThe model supports a learnable stop latent vector for generation control.\n\n```python\ndef _init_stop_latent(latent_seq_len: int, latent_dim: int,\n                      init: str = \"random_normalized\",\n                      seed: int = 1337) -> torch.Tensor\n```\n\nInitialization options:\n- zero: All zeros\n- random_normalized: Random unit vectors\n\nThe stop latent is:\n- Learned during training\n- Appended to batches for augmentation\n- Can signal end-of-generation", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "Stop Latent Mechanism", "start_line": 166, "end_line": 184}}
{"id": "models.md_Complete Data Flow_0", "text": "## Complete Data Flow\n\n```\nInput Text (Dialogue)\n    |\nModernBERT Encoder       [B, L, 768]\n    |\nMean Pooling             [B, 768]\n    |\nCompression MLP          [B, 8, 256] (z_in)\n    |\nMiddleTransformer        [B, 8, 256] (z_out)\n    |      (4 layers, self-attention)\n    |\nPrefix Adapter (Cross)   [B, 50, 640]\n    |\nGemma-3-270m Decoder\n    |\nOutput Text (Summary)\n```", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "Complete Data Flow", "start_line": 185, "end_line": 205}}
{"id": "models.md_Key Design Decisions_0", "text": "## Key Design Decisions\n\n1. Multiple latent vectors (8) instead of single bottleneck\n2. Self-attention across latent positions for information exchange\n3. 4 transformer layers for multi-step reasoning\n4. Cross-attention for latent-to-decoder mapping\n5. RoPE for position-aware latent transformation\n6. RMSNorm and SwiGLU for modern transformer architecture", "metadata": {"source": "models.md", "title": "Model Architecture Documentation", "section": "Key Design Decisions", "start_line": 206, "end_line": 214}}
{"id": "README.md_Latent-Space Reasoning Model_0", "text": "# Latent-Space Reasoning Model\n\nA PyTorch prototype for a model that \"thinks\" in a compact latent \"idea space\" instead of directly in token space.", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Latent-Space Reasoning Model", "start_line": 1, "end_line": 4}}
{"id": "README.md_Architecture_0", "text": "## Architecture\n\nThe model consists of three main components:\n\n1. **Encoder (ModernBERT)**: Converts text tokens to a compact \"idea vector\" (latent_dim=256)\n2. **Middle Model (MLP)**: Transforms idea vectors through latent reasoning (2-4 layer MLP)\n3. **Decoder (GPT-2)**: Converts transformed idea back to text using a prefix adapter", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Architecture", "start_line": 5, "end_line": 12}}
{"id": "README.md_Data Flow_0", "text": "### Data Flow\n\n```\nInput Text -> ModernBERT -> Pooling -> Compression MLP -> Idea Vector (256-dim)\n                                                          |\n                                                    Middle Model (MLP)\n                                                          |\n                                                    Transformed Idea\n                                                          |\n                                            Prefix Adapter (Expansion MLP)\n                                                          |\n                                              GPT-2 Prefix Embeddings\n                                                          |\n                                                    Generated Text\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Data Flow", "start_line": 13, "end_line": 28}}
{"id": "README.md_Installation_0", "text": "## Installation\n\n```bash\npip install -r requirements.txt\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Installation", "start_line": 29, "end_line": 34}}
{"id": "README.md_Usage_0", "text": "## Usage", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Usage", "start_line": 35, "end_line": 36}}
{"id": "README.md_Training_0", "text": "### Training\n\nTrain the model on the SAMSum dataset:\n\n```bash\npython main.py --mode train\n```\n\nWith custom hyperparameters:\n\n```bash\npython main.py --mode train \\\n    --batch-size 8 \\\n    --learning-rate 1e-4 \\\n    --epochs 5 \\\n    --max-train-samples 10000\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Training", "start_line": 37, "end_line": 54}}
{"id": "README.md_Interactive Inference_0", "text": "### Interactive Inference\n\nRun interactive inference session:\n\n```bash\npython main.py --mode inference\n```\n\nOr specify a checkpoint:\n\n```bash\npython main.py --mode inference --checkpoint checkpoints/best_model.pt\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Interactive Inference", "start_line": 55, "end_line": 68}}
{"id": "README.md_Batch Generation_0", "text": "### Batch Generation\n\nGenerate text from input:\n\n```bash\npython main.py --mode generate \\\n    --input \"Your input text here\" \\\n    --checkpoint checkpoints/best_model.pt\n```\n\nWith generation parameters:\n\n```bash\npython main.py --mode generate \\\n    --input \"Your input text here\" \\\n    --checkpoint checkpoints/best_model.pt \\\n    --max-length 128 \\\n    --temperature 0.7 \\\n    --top-p 0.9\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Batch Generation", "start_line": 69, "end_line": 89}}
{"id": "README.md_Project Structure_0", "text": "## Project Structure\n\n```\nLatent-Space-Model/\n\u251c\u2500\u2500 main.py              # Entry point with CLI\n\u251c\u2500\u2500 requirements.txt     # Dependencies\n\u251c\u2500\u2500 README.md           # This file\n\u251c\u2500\u2500 stack.md            # Architecture specification\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 config.py       # Configuration and hyperparameters\n\u2502   \u251c\u2500\u2500 models.py       # Model architecture\n\u2502   \u251c\u2500\u2500 data.py         # Data loading (SAMSum)\n\u2502   \u251c\u2500\u2500 train.py        # Training loop\n\u2502   \u2514\u2500\u2500 inference.py    # Inference and generation\n\u2514\u2500\u2500 checkpoints/        # Saved model checkpoints (created during training)\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Project Structure", "start_line": 90, "end_line": 107}}
{"id": "README.md_Configuration_0", "text": "## Configuration\n\nKey hyperparameters (defined in src/config.py):\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| latent_dim | 256 | Dimension of idea vector |\n| prefix_len | 10 | Length of GPT-2 prefix |\n| modernbert_hidden_dim | 768 | ModernBERT hidden size |\n| gpt2_hidden_dim | 768 | GPT-2 hidden size |\n| middle_hidden_dim | 512 | Middle model hidden size |\n| middle_layers | 3 | Number of middle MLP layers |\n| learning_rate | 1e-4 | Learning rate |\n| batch_size | 4 | Batch size |\n| max_seq_len | 256 | Maximum input sequence length |\n| max_target_len | 128 | Maximum target sequence length |", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Configuration", "start_line": 108, "end_line": 124}}
{"id": "README.md_Model Components_0", "text": "## Model Components", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Model Components", "start_line": 125, "end_line": 126}}
{"id": "README.md_LatentEncoder_0", "text": "### LatentEncoder\n\n- Uses ModernBERT (answerdotai/ModernBERT-base) as backbone\n- Mean pooling over non-padded tokens\n- Compression MLP: 768 -> 512 -> 256\n- ModernBERT backbone is frozen during training", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "LatentEncoder", "start_line": 127, "end_line": 133}}
{"id": "README.md_MiddleModel_0", "text": "### MiddleModel\n\n- 3-layer MLP for latent reasoning\n- Architecture: 256 -> 512 -> 512 -> 256\n- Fully trainable", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "MiddleModel", "start_line": 134, "end_line": 139}}
{"id": "README.md_PrefixAdapter_0", "text": "### PrefixAdapter\n\n- Expands latent vector to GPT-2 prefix embeddings\n- Architecture: 256 -> (10 x 768)\n- Fully trainable", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "PrefixAdapter", "start_line": 140, "end_line": 145}}
{"id": "README.md_LatentSpaceModel_0", "text": "### LatentSpaceModel\n\n- Combines all components\n- GPT-2 (gpt2) backbone is frozen during training\n- Only trainable: encoder compression, middle model, prefix adapter", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "LatentSpaceModel", "start_line": 146, "end_line": 151}}
{"id": "README.md_Training Strategy_0", "text": "## Training Strategy\n\n- **Frozen backbones**: ModernBERT and GPT-2 are frozen to save memory\n- **Trainable components**: Encoder compression MLP, middle model, prefix adapter\n- **Loss**: Cross-entropy on decoder outputs\n- **Optimizer**: AdamW with weight decay\n- **Mixed precision**: FP16 for memory efficiency\n- **Gradient accumulation**: Supported for effective larger batch sizes", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Training Strategy", "start_line": 152, "end_line": 160}}
{"id": "README.md_Dataset_0", "text": "## Dataset\n\nUses the SAMSum dataset for dialogue summarization:\n- ~14,700 dialogue-summary pairs\n- Loaded from Hugging Face Datasets\n- Suitable for prototyping on RTX 3060", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Dataset", "start_line": 161, "end_line": 167}}
{"id": "README.md_Hardware Requirements_0", "text": "## Hardware Requirements\n\n- GPU: RTX 3060 (12GB VRAM) or equivalent\n- RAM: 16GB+ recommended\n- Storage: ~5GB for models and checkpoints", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Hardware Requirements", "start_line": 168, "end_line": 173}}
{"id": "README.md_Features_0", "text": "## Features\n\n- ModernBERT encoder with frozen backbone\n- MLP-based latent reasoning core\n- GPT-2 decoder with prefix adapter\n- Mixed precision training (FP16)\n- Gradient accumulation support\n- Checkpoint saving and loading\n- Interactive inference mode\n- Batch text generation\n- Latent vector interpolation\n- Configurable hyperparameters", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Features", "start_line": 174, "end_line": 186}}
{"id": "README.md_Advanced Features_0", "text": "## Advanced Features", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Advanced Features", "start_line": 187, "end_line": 188}}
{"id": "README.md_Latent Interpolation_0", "text": "### Latent Interpolation\n\nExplore the latent space by interpolating between two inputs:\n\n```python\nfrom src.inference import LatentSpaceInference\nfrom src.config import Config\n\nconfig = Config()\ninference = LatentSpaceInference(\"checkpoints/best_model.pt\", config)", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Latent Interpolation", "start_line": 189, "end_line": 199}}
{"id": "README.md_Interpolate between two texts_0", "text": "# Interpolate between two texts\nresult = inference.interpolate_and_generate(\n    input_text_1=\"First dialogue...\",\n    input_text_2=\"Second dialogue...\",\n    alpha=0.5  # 0.0 = first, 1.0 = second\n)\nprint(result)\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Interpolate between two texts", "start_line": 200, "end_line": 208}}
{"id": "README.md_Get Latent Vectors_0", "text": "### Get Latent Vectors\n\nExtract latent representations for analysis:\n\n```python\nlatent = inference.get_latent_vector(\"Your text here\")\nprint(f\"Latent shape: {latent.shape}\")  # torch.Size([256])\n```", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "Get Latent Vectors", "start_line": 209, "end_line": 217}}
{"id": "README.md_References_0", "text": "## References\n\n- stack.md - Detailed architecture specification\n- ModernBERT: answerdotai/ModernBERT-base\n- GPT-2: gpt2\n- SAMSum Dataset: samsum from Hugging Face", "metadata": {"source": "README.md", "title": "Latent-Space Reasoning Model", "section": "References", "start_line": 218, "end_line": 224}}
